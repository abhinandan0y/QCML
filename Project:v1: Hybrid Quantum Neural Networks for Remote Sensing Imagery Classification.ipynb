{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install qiskit pennylane tensorflow numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70863320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "#### Step 2: Prepare the Data\n",
    "# Load EuroSAT dataset\n",
    "dataset, info = tfds.load('eurosat/rgb', with_info=True)\n",
    "#train_data = dataset['train']\n",
    "\n",
    "train_data, val_data = tfds.load('eurosat/rgb', split=['train[:80%]', 'train[80%:]'], as_supervised=True)\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "#def preprocess(features):\n",
    "#    image = tf.image.resize(features['image'], (64, 64)) / 255.0\n",
    "#    label = features['label']\n",
    "#    return image, label\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (64, 64)) / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_data = train_data.map(preprocess).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "#### Step 3: Define the Quantum Layer\n",
    "n_qubits = 4\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def quantum_circuit(inputs):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(inputs[i], wires=i)\n",
    "    qml.CZ(wires=[0, 1])\n",
    "    qml.CZ(wires=[2, 3])\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(inputs[i], wires=i)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "def quantum_layer(inputs):\n",
    "    inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "    outputs = np.array([quantum_circuit(input) for input in inputs])\n",
    "    outputs = outputs.astype(np.float32)  # Ensure the numpy array is float32\n",
    "    return tf.convert_to_tensor(outputs, dtype=tf.float32)\n",
    "\n",
    "#### Step 4: Define the Model\n",
    "class HybridModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(32, (3, 3), activation='relu')\n",
    "        self.pool1 = layers.MaxPooling2D((2, 2))\n",
    "        self.conv2 = layers.Conv2D(64, (3, 3), activation='relu')\n",
    "        self.pool2 = layers.MaxPooling2D((2, 2))\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(n_qubits, activation='relu')\n",
    "        self.dense2 = layers.Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = tf.numpy_function(quantum_layer, [x], tf.float32)\n",
    "        x.set_shape((None, n_qubits))  # Ensure shape is set for the output of quantum layer\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "model = HybridModel()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Custom training loop\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch+1}')\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "        if step % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} Step {step} Loss {loss_value.numpy()} Accuracy {train_acc_metric.result().numpy()}')\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(f'Training accuracy over epoch {epoch+1}: {train_acc.numpy()}')\n",
    "    train_acc_metric.reset_states()\n",
    "    \n",
    "#### Step 5: Evaluate the Model\n",
    "\n",
    "# Preprocess the validation data\n",
    "#val_data = val_data.map(preprocess).batch(32)\n",
    "val_data = val_data.map(lambda image, label: preprocess(image, label)).batch(32)\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "val_loss, val_accuracy = model.evaluate(val_data)\n",
    "\n",
    "# Print the validation loss and accuracy\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "169/169 [==============================] - 65s 381ms/step - loss: 2.2502 - accuracy: 0.1667\n",
    ">>> # Print the validation loss and accuracy\n",
    ">>> print(\"Validation Loss:\", val_loss)\n",
    "Validation Loss: 2.250182867050171\n",
    ">>> print(\"Validation Accuracy:\", val_accuracy)\n",
    "Validation Accuracy: 0.1666666716337204\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ca700",
   "metadata": {},
   "outputs": [],
   "source": [
    "$#####################  Potential Improvements:\n",
    "\n",
    "#Data Augmentation:\n",
    "    Incorporate data augmentation techniques like rotation, flipping, and scaling to increase the diversity of the training dataset. This can help improve model generalization.\n",
    "\n",
    "#Complexity of Quantum Circuit: \n",
    "    Experiment with different quantum circuit architectures, including varying the number of qubits, layers, and types of gates. More complex quantum circuits may capture richer features from the data.\n",
    "\n",
    "#Hyperparameter Tuning: \n",
    "    Tune hyperparameters such as learning rate, batch size, and the number of epochs to find the optimal configuration for your model.\n",
    "\n",
    "#Regularization: \n",
    "    Apply regularization techniques like dropout or L2 regularization to prevent overfitting and improve model robustness.\n",
    "\n",
    "#Ensemble Learning: \n",
    "    Train multiple hybrid models with different initializations or architectures and combine their predictions to boost performance.\n",
    "\n",
    "#Quantum Embedding: \n",
    "    Explore methods to embed classical data into quantum states more effectively, such as amplitude encoding or quantum feature maps.\n",
    "\n",
    "#Transfer Learning: \n",
    "    Utilize pre-trained classical convolutional neural networks (CNNs) as feature extractors before passing the features to the quantum layer.\n",
    "\n",
    "#Monitor Training Progress: \n",
    "    Visualize training metrics like loss and accuracy over epochs using tools like TensorBoard to identify potential issues or areas for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
